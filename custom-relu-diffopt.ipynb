{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function myRelu(y::AbstractArray{T}, model::JuMP.Model) where {T}\n",
    "#     N = length(y)\n",
    "    \n",
    "#     empty!(model)\n",
    "#     set_optimizer_attribute(model, MOI.Silent(), true)\n",
    "#     @variable(model, x[1:N] >= zero(T))\n",
    "#     x = model[:x]\n",
    "#     @objective(\n",
    "#         model,\n",
    "#         Min,\n",
    "#         x'x -2x'y + y'y,\n",
    "#     )\n",
    "#     optimize!(model)\n",
    "\n",
    "#     return value.(x)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using DiffOpt\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "using OSQP\n",
    "using JuMP\n",
    "using ChainRulesCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Flux's datasets are deprecated, please use the package MLDatasets.jl\n",
      "└ @ Flux.Data /home/pika/.julia/packages/Flux/6o4DQ/src/data/Data.jl:17\n",
      "┌ Warning: Flux's datasets are deprecated, please use the package MLDatasets.jl\n",
      "└ @ Flux.Data /home/pika/.julia/packages/Flux/6o4DQ/src/data/Data.jl:17\n",
      "┌ Warning: Flux's datasets are deprecated, please use the package MLDatasets.jl\n",
      "└ @ Flux.Data /home/pika/.julia/packages/Flux/6o4DQ/src/data/Data.jl:17\n",
      "┌ Warning: Flux's datasets are deprecated, please use the package MLDatasets.jl\n",
      "└ @ Flux.Data /home/pika/.julia/packages/Flux/6o4DQ/src/data/Data.jl:17\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "imgs = Flux.Data.MNIST.images()\n",
    "labels = Flux.Data.MNIST.labels();\n",
    "\n",
    "# Preprocessing\n",
    "X = hcat(float.(reshape.(imgs, :))...) #stack all the images\n",
    "Y = onehotbatch(labels, 0:9); # just a common way to encode categorical variables\n",
    "\n",
    "test_X = hcat(float.(reshape.(Flux.Data.MNIST.images(:test), :))...)\n",
    "test_Y = onehotbatch(Flux.Data.MNIST.labels(:test), 0:9)\n",
    "\n",
    "# float64 to float16, to save memory\n",
    "X = convert(Array{Float16,2}, X) \n",
    "test_X = convert(Array{Float16,2}, test_X)\n",
    "\n",
    "X = X[:, 1:1000]\n",
    "Y = Y[:, 1:1000];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myRelu"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    relu method for a Matrix\n",
    "\"\"\"\n",
    "function myRelu(y::AbstractMatrix{T}; model = Model(() -> diff_optimizer(OSQP.Optimizer))) where {T}\n",
    "    x̂ = zero(y)\n",
    "    \n",
    "    # model init\n",
    "    N = length(y[:, 1])\n",
    "    empty!(model)\n",
    "    set_optimizer_attribute(model, MOI.Silent(), true)\n",
    "    @variable(model, x[1:N] >= zero(T))\n",
    "    \n",
    "    for i in 1:size(y)[2]\n",
    "        @objective(\n",
    "            model,\n",
    "            Min,\n",
    "            x'x -2x'y[:, i]\n",
    "        )\n",
    "        optimize!(model)\n",
    "        x̂[:, i] = value.(x)\n",
    "    end\n",
    "    return x̂\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ChainRulesCore.rrule(::typeof(myRelu), y::AbstractArray{T}; model = Model(() -> diff_optimizer(OSQP.Optimizer))) where {T}\n",
    "    \n",
    "    pv = myRelu(y, model=model) \n",
    "    \n",
    "    function pullback_myRelu(dx)\n",
    "        x = model[:x]\n",
    "        dy = zero(dx)\n",
    "        \n",
    "        for i in 1:size(y)[2]\n",
    "            MOI.set.(\n",
    "                model,\n",
    "                DiffOpt.BackwardIn{MOI.VariablePrimal}(), \n",
    "                x,\n",
    "                dx[:, i]\n",
    "            ) \n",
    "\n",
    "            DiffOpt.backward(model)  # find grad\n",
    "\n",
    "            dy[:, i] = MOI.get.(\n",
    "                model,\n",
    "                DiffOpt.BackwardOut{DiffOpt.LinearObjective}(), \n",
    "                x, \n",
    "            )  # coeff of `x` in -2x'y\n",
    "            dy[:, i] = -2 * dy[:, i]\n",
    "        end\n",
    "        \n",
    "        return (NO_FIELDS, dy)\n",
    "    end\n",
    "    return pv, pullback_myRelu\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(784, 64), myRelu, Dense(64, 10), softmax)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    Dense(784, 64),\n",
    "    myRelu,\n",
    "    Dense(64, 10),\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#14 (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x, y) = crossentropy(m(x), y) \n",
    "opt = ADAM(); # popular stochastic gradient descent variant\n",
    "\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y)) # cute way to find average of correct guesses\n",
    "\n",
    "dataset = repeated((X,Y), 20) # repeat the data set, very low accuracy on the orig dataset\n",
    "evalcb = () -> @show(loss(X, Y)) # callback to show loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(X, Y) = 2.2714f0\n",
      "loss(X, Y) = 2.076394f0\n",
      "loss(X, Y) = 1.9149227f0\n",
      "loss(X, Y) = 1.7748072f0\n",
      "loss(X, Y) = 1.6523327f0\n",
      "accuracy(X, Y) = 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The addition operator has been used on JuMP expressions a large number of times. This warning is safe to ignore but may indicate that model generation is slower than necessary. For performance reasons, you should not add expressions in a loop. Instead of x += y, use add_to_expression!(x,y) to modify x in place. If y is a single variable, you may also use add_to_expression!(x, coef, y) for x += coef*y.\n",
      "└ @ JuMP /home/pika/.julia/packages/JuMP/MIPb6/src/JuMP.jl:1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_X, test_Y) = 0.6635\n"
     ]
    }
   ],
   "source": [
    "Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 5)); #took me ~5 minutes to train on CPU\n",
    "\n",
    "@show accuracy(X,Y)\n",
    "@show accuracy(test_X, test_Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  accuracy \n",
    "- Dense(..., relu) ~ 0.875\n",
    "- Dense(...), myRelu ~ 0.543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
